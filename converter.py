import requests
import xml.etree.ElementTree as ET
from dotenv import load_dotenv
import os
import re
from marker.converters.pdf import PdfConverter
from marker.models import create_model_dict
from marker.output import text_from_rendered
from marker.config.parser import ConfigParser
import torch
import time
import logging
from datetime import datetime, timedelta
from ftplib import FTP
import psutil
import gc
import subprocess
import json
import sys
import fcntl
import atexit
from urllib.parse import unquote


# ============================================
# CONFIGURATION DU LOGGING
# ============================================

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("logs.log", mode="a"),
        logging.StreamHandler()
    ]
)

start_time = time.time()

# Charger les variables depuis .env
load_dotenv()

# ============================================
# VARIABLES D'ENVIRONNEMENT
# ============================================

SITEMAP_URL = os.getenv("SITEMAP_URL")
LOCAL_SITEMAP_FILE = os.getenv("LOCAL_SITEMAP_FILE")
DOWNLOAD_FOLDER = os.getenv("DOWNLOAD_FOLDER")
MARKDOWN_FOLDER = os.getenv("MARKDOWN_FOLDER")
FTP_HOST = os.getenv("FTP_HOST")
FTP_USER = os.getenv("FTP_USER")
FTP_PASS = os.getenv("FTP_PASS")
FTP_DIR = "/markdown"
FAILED_PDF_LOG = "failed_pdfs.json"
PROCESSED_PDF_LOG = "processed_pdfs.json"
REMOVED_PDF_LOG = "removed_pdfs.json"  # Nouveau: tracking des PDFs supprim√©s
CHATBOT_ID = os.getenv("CHATBOT_ID")
BEARER_TOKEN = os.getenv("BEARER_TOKEN")
BASE_URL = os.getenv("BASE_URL")
LOCK_FILE = "/tmp/converter.lock"

# Timeouts (en secondes)
HTTP_TIMEOUT = 60
FTP_TIMEOUT = 120

HEADERS = {
    "Authorization": f"Bearer {BEARER_TOKEN}",
    "Accept": "application/json",
    "Content-Type": "application/json"
}

# Variable globale pour le converter (r√©utilisation)
_converter = None
_lock_fd = None


# ============================================
# GESTION DU LOCK (√©viter ex√©cutions concurrentes)
# ============================================

def acquire_lock():
    """Acquiert un lock exclusif pour √©viter les ex√©cutions concurrentes"""
    global _lock_fd
    try:
        _lock_fd = open(LOCK_FILE, 'w')
        fcntl.flock(_lock_fd.fileno(), fcntl.LOCK_EX | fcntl.LOCK_NB)
        _lock_fd.write(str(os.getpid()))
        _lock_fd.flush()
        logging.info("Lock acquis avec succ√®s")
        return True
    except (IOError, OSError) as e:
        logging.error(f"Impossible d'acqu√©rir le lock - une autre instance tourne ? {e}")
        return False


def release_lock():
    """Lib√®re le lock"""
    global _lock_fd
    if _lock_fd:
        try:
            fcntl.flock(_lock_fd.fileno(), fcntl.LOCK_UN)
            _lock_fd.close()
            if os.path.exists(LOCK_FILE):
                os.remove(LOCK_FILE)
            logging.info("Lock lib√©r√©")
        except Exception as e:
            logging.warning(f"Erreur lors de la lib√©ration du lock: {e}")


# ============================================
# UTILITAIRES
# ============================================

def get_clean_filename(url):
    """Extrait et nettoie le nom de fichier depuis l'URL (fonction centralis√©e)"""
    raw_filename = url.split("&ind=")[-1]
    # Decode URL encoding si pr√©sent
    raw_filename = unquote(raw_filename)
    # Supprime le pr√©fixe num√©rique wpdm_ si pr√©sent
    return re.sub(r"^\d+wpdm_", "", raw_filename)


def get_markdown_filename(pdf_url):
    """Retourne le nom du fichier markdown pour un PDF donn√©"""
    clean_filename = get_clean_filename(pdf_url)
    return clean_filename.replace(".pdf", ".md")


def get_markdown_path(pdf_url):
    """Retourne le chemin complet du fichier markdown pour un PDF donn√©"""
    return os.path.join(MARKDOWN_FOLDER, get_markdown_filename(pdf_url))


def suspendInstance():
    """Suspend l'instance OVH"""
    max_retries = 3
    for attempt in range(max_retries):
        try:
            result = subprocess.run(
                ["python", "suspendInstance.py"], 
                check=True, 
                capture_output=True, 
                text=True,
                timeout=60
            )
            logging.info(f"Script suspendInstance ex√©cut√© avec succ√®s : {result.stdout}")
            return
        except subprocess.TimeoutExpired:
            logging.error(f"Timeout lors de l'ex√©cution de suspendInstance.py (tentative {attempt + 1}/{max_retries})")
        except subprocess.CalledProcessError as e:
            logging.error(f"Erreur lors de l'ex√©cution de suspendInstance.py (tentative {attempt + 1}/{max_retries}): {e.stderr}")
        
        if attempt < max_retries - 1:
            time.sleep(10)
    
    logging.critical("Impossible de suspendre l'instance apr√®s plusieurs tentatives")


def check_memory_usage():
    """V√©rifie l'utilisation m√©moire et alerte si critique"""
    mem = psutil.virtual_memory()
    gpu_mem_used = 0
    
    if torch.cuda.is_available():
        gpu_mem_used = torch.cuda.memory_allocated() / 1024**3  # En GB
        
    logging.info(f"M√©moire RAM: {mem.percent}% | GPU: {gpu_mem_used:.2f} GB")
    
    if mem.percent > 85:
        logging.warning("‚ö†Ô∏è ALERTE: M√©moire RAM tr√®s haute (>85%) - risque d'arr√™t!")
        # Tenter de lib√©rer de la m√©moire
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        upload_to_ftp("logs.log")
        return False
    
    return True


def cleanup_gpu_memory():
    """Nettoie la m√©moire GPU de mani√®re agressive"""
    global _converter
    
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.synchronize()
    
    gc.collect()
    
    # R√©initialiser le converter si la m√©moire est trop utilis√©e
    if torch.cuda.is_available():
        mem_allocated = torch.cuda.memory_allocated() / 1024**3
        if mem_allocated > 2.0:  # Plus de 2GB utilis√©s
            logging.warning(f"M√©moire GPU √©lev√©e ({mem_allocated:.2f}GB), r√©initialisation du converter")
            _converter = None
            gc.collect()
            torch.cuda.empty_cache()


# ============================================
# FONCTIONS FTP
# ============================================

def upload_to_ftp(file_path, max_retries=3):
    """Upload un fichier vers le serveur FTP avec retry"""
    if not os.path.exists(file_path):
        logging.warning(f"Fichier inexistant, upload ignor√©: {file_path}")
        return False
    
    for attempt in range(max_retries):
        try:
            with FTP(FTP_HOST, timeout=FTP_TIMEOUT) as ftp:
                ftp.login(FTP_USER, FTP_PASS)
                ftp.cwd(FTP_DIR)
                with open(file_path, "rb") as f:
                    ftp.storbinary(f"STOR {os.path.basename(file_path)}", f)
                logging.info(f"Upload FTP r√©ussi : {file_path} -> {FTP_DIR}")
                return True
        except Exception as e:
            logging.error(f"√âchec upload FTP (tentative {attempt + 1}/{max_retries}): {e}")
            if attempt < max_retries - 1:
                time.sleep(5)
    
    logging.error(f"Upload FTP d√©finitivement √©chou√© pour {file_path}")
    return False


def delete_from_ftp(filename, max_retries=3):
    """Supprime un fichier du serveur FTP avec retry"""
    for attempt in range(max_retries):
        try:
            with FTP(FTP_HOST, timeout=FTP_TIMEOUT) as ftp:
                ftp.login(FTP_USER, FTP_PASS)
                ftp.cwd(FTP_DIR)
                
                # V√©rifier si le fichier existe
                file_list = ftp.nlst()
                if filename not in file_list:
                    logging.info(f"Fichier FTP d√©j√† absent: {filename}")
                    return True
                
                # Supprimer le fichier
                ftp.delete(filename)
                logging.info(f"Fichier FTP supprim√©: {filename}")
                return True
                
        except Exception as e:
            logging.error(f"√âchec suppression FTP (tentative {attempt + 1}/{max_retries}): {e}")
            if attempt < max_retries - 1:
                time.sleep(5)
    
    logging.error(f"Suppression FTP d√©finitivement √©chou√©e pour {filename}")
    return False


def list_ftp_files():
    """Liste tous les fichiers sur le serveur FTP"""
    try:
        with FTP(FTP_HOST, timeout=FTP_TIMEOUT) as ftp:
            ftp.login(FTP_USER, FTP_PASS)
            ftp.cwd(FTP_DIR)
            return ftp.nlst()
    except Exception as e:
        logging.error(f"Erreur lors du listing FTP: {e}")
        return []


# ============================================
# FONCTIONS CHATBOT API
# ============================================

def get_sources():
    """R√©cup√®re toutes les sources du chatbot"""
    try:
        response = requests.get(
            f"{BASE_URL}/chatbot/{CHATBOT_ID}/sources", 
            headers=HEADERS,
            timeout=HTTP_TIMEOUT
        )
        if response.status_code == 200:
            return response.json()
        logging.error(f"Erreur API sources {response.status_code}: {response.text}")
    except requests.exceptions.Timeout:
        logging.error("Timeout lors de la r√©cup√©ration des sources")
    except requests.exceptions.RequestException as e:
        logging.error(f"Erreur r√©seau lors de la r√©cup√©ration des sources: {e}")
    return None


def find_source_by_keyword(sources, keyword):
    """Recherche une source contenant un mot-cl√© dans son URL"""
    if not sources:
        return None
    return next((s for s in sources if keyword in s.get("url", "")), None)


def delete_source(source_id):
    """Supprime une source sp√©cifique"""
    try:
        response = requests.delete(
            f"{BASE_URL}/sources/{source_id}", 
            headers=HEADERS,
            timeout=HTTP_TIMEOUT
        )
        if response.status_code in [200, 204, 404]:
            logging.info(f"Source supprim√©e ou introuvable : {source_id}")
            return True
        logging.error(f"Erreur suppression source {response.status_code}: {response.text}")
    except requests.exceptions.RequestException as e:
        logging.error(f"Erreur r√©seau lors de la suppression: {e}")
    return False


def read_markdown_content(pdf_url):
    """Lit le contenu du fichier markdown correspondant au PDF"""
    md_path = get_markdown_path(pdf_url)
    
    if os.path.exists(md_path):
        try:
            with open(md_path, "r", encoding="utf-8") as file:
                content = file.read()
                if content.strip():
                    return content
                logging.warning(f"Fichier Markdown vide: {md_path}")
        except Exception as e:
            logging.error(f"Erreur lecture Markdown {md_path}: {e}")
    else:
        logging.warning(f"Fichier Markdown introuvable: {md_path}")
    
    return ""


def create_source(url, markdown_content):
    """Ajoute une nouvelle source avec l'URL et le contenu Markdown"""
    try:
        payload = {"url": url, "content": markdown_content}
        response = requests.post(
            f"{BASE_URL}/chatbot/{CHATBOT_ID}/sources", 
            headers=HEADERS, 
            json=payload,
            timeout=HTTP_TIMEOUT
        )
        
        if response.status_code in [200, 201]:
            logging.info(f"Source ajout√©e : {url}")
            return True
        logging.error(f"Erreur cr√©ation source {response.status_code}: {response.text}")
    except requests.exceptions.RequestException as e:
        logging.error(f"Erreur r√©seau lors de la cr√©ation: {e}")
    return False


def verify_source_added(keyword, max_retries=3):
    """V√©rifie que la nouvelle source a bien √©t√© ajout√©e"""
    for attempt in range(max_retries):
        time.sleep(2)  # Attendre que l'API se synchronise
        sources = get_sources()
        if sources and find_source_by_keyword(sources, keyword):
            logging.info("V√©rification r√©ussie : la source est bien pr√©sente")
            return True
        if attempt < max_retries - 1:
            logging.info(f"V√©rification en cours... (tentative {attempt + 2}/{max_retries})")
    
    logging.error("V√©rification √©chou√©e : la source n'a pas √©t√© ajout√©e")
    return False


def process_chatbot_source(pdf_url):
    """G√®re l'ajout/mise √† jour de la source dans le chatbot"""
    clean_filename = get_clean_filename(pdf_url)
    
    sources = get_sources()
    if sources is None:
        raise Exception("Impossible de r√©cup√©rer les sources du chatbot")

    # Chercher si une source existe d√©j√†
    source_to_reset = find_source_by_keyword(sources, clean_filename)
    if source_to_reset:
        source_id = source_to_reset["id"]
        logging.info(f"Source existante trouv√©e : {source_id}")
        if not delete_source(source_id):
            logging.warning(f"√âchec suppression source {source_id}, tentative de cr√©ation quand m√™me")
    
    # Lire le contenu markdown
    markdown_content = read_markdown_content(pdf_url)
    if not markdown_content:
        raise Exception(f"Contenu Markdown vide ou inexistant pour {pdf_url}")
    
    # Cr√©er la nouvelle source
    if not create_source(pdf_url, markdown_content):
        raise Exception(f"√âchec de cr√©ation de la source pour {pdf_url}")
    
    # V√©rifier l'ajout
    if not verify_source_added(clean_filename):
        raise Exception(f"Source non v√©rifi√©e pour {pdf_url}")
    
    logging.info(f"‚úÖ Source chatbot mise √† jour pour {clean_filename}")


# ============================================
# FONCTIONS DE TRACKING (JSON)
# ============================================

def load_processed_pdfs():
    """Charge le dictionnaire des PDFs d√©j√† trait√©s avec leur date de traitement"""
    if os.path.exists(PROCESSED_PDF_LOG):
        try:
            with open(PROCESSED_PDF_LOG, "r", encoding="utf-8") as f:
                return json.load(f)
        except json.JSONDecodeError:
            logging.warning("Fichier processed_pdfs.json corrompu, cr√©ation d'un nouveau")
            return {}
    return {}


def save_processed_pdfs(processed):
    """Sauvegarde le dictionnaire des PDFs trait√©s"""
    with open(PROCESSED_PDF_LOG, "w", encoding="utf-8") as f:
        json.dump(processed, f, indent=2, ensure_ascii=False)


def save_processed_pdf(url, date):
    """Enregistre un PDF comme trait√© avec sa date"""
    processed = load_processed_pdfs()
    processed[url] = {
        "date": date,
        "processed_at": datetime.now().isoformat(),
        "filename": get_clean_filename(url)
    }
    save_processed_pdfs(processed)
    logging.info(f"PDF enregistr√© comme trait√© : {get_clean_filename(url)}")


def remove_processed_pdf(url):
    """Retire un PDF de la liste des PDFs trait√©s"""
    processed = load_processed_pdfs()
    if url in processed:
        del processed[url]
        save_processed_pdfs(processed)
        return True
    return False


def is_pdf_already_processed(url, current_date):
    """V√©rifie si un PDF a d√©j√† √©t√© trait√© avec cette date"""
    processed = load_processed_pdfs()
    if url in processed:
        return processed[url].get("date") == current_date
    return False


def load_failed_pdfs():
    """Charge les PDFs √©chou√©s avec leur date d'√©chec"""
    if os.path.exists(FAILED_PDF_LOG):
        try:
            with open(FAILED_PDF_LOG, "r", encoding="utf-8") as f:
                return json.load(f)
        except json.JSONDecodeError:
            logging.warning("Fichier failed_pdfs.json corrompu")
            return {}
    return {}


def save_failed_pdfs(failed):
    """Sauvegarde le dictionnaire des PDFs √©chou√©s"""
    with open(FAILED_PDF_LOG, "w", encoding="utf-8") as f:
        json.dump(failed, f, indent=2, ensure_ascii=False)


def save_failed_pdf(url, error_msg):
    """Enregistre un PDF √©chou√©"""
    failed = load_failed_pdfs()
    retry_count = failed.get(url, {}).get("retry_count", 0) + 1
    failed[url] = {
        "error": str(error_msg)[:500],  # Limiter la taille de l'erreur
        "failed_at": datetime.now().isoformat(),
        "retry_count": retry_count,
        "filename": get_clean_filename(url)
    }
    save_failed_pdfs(failed)
    logging.info(f"PDF enregistr√© comme √©chou√© (tentative {retry_count}): {get_clean_filename(url)}")


def remove_from_failed(url):
    """Retire un PDF de la liste des √©checs apr√®s succ√®s"""
    failed = load_failed_pdfs()
    if url in failed:
        del failed[url]
        save_failed_pdfs(failed)
        logging.info(f"PDF retir√© de la liste des √©checs : {get_clean_filename(url)}")


def should_retry_failed_pdf(url, max_retries=3, retry_after_days=7):
    """V√©rifie si un PDF √©chou√© devrait √™tre r√©essay√©"""
    failed = load_failed_pdfs()
    if url not in failed:
        return True
    
    retry_count = failed[url].get("retry_count", 0)
    if retry_count >= max_retries:
        logging.debug(f"PDF {get_clean_filename(url)} ignor√©: max retries atteint ({retry_count})")
        return False
    
    try:
        failed_at = datetime.fromisoformat(failed[url]["failed_at"])
        if datetime.now() - failed_at > timedelta(days=retry_after_days):
            logging.info(f"PDF {get_clean_filename(url)} √©ligible pour retry (dernier √©chec > {retry_after_days} jours)")
            return True
    except (KeyError, ValueError):
        return True
    
    return False


def load_removed_pdfs():
    """Charge l'historique des PDFs supprim√©s"""
    if os.path.exists(REMOVED_PDF_LOG):
        try:
            with open(REMOVED_PDF_LOG, "r", encoding="utf-8") as f:
                return json.load(f)
        except json.JSONDecodeError:
            return {}
    return {}


def save_removed_pdf(url, cleanup_result):
    """Enregistre un PDF comme supprim√© avec le r√©sultat du nettoyage"""
    removed = load_removed_pdfs()
    removed[url] = {
        "filename": get_clean_filename(url),
        "removed_at": datetime.now().isoformat(),
        "cleanup": cleanup_result
    }
    with open(REMOVED_PDF_LOG, "w", encoding="utf-8") as f:
        json.dump(removed, f, indent=2, ensure_ascii=False)


# ============================================
# FONCTIONS SITEMAP
# ============================================

def download_sitemap():
    """T√©l√©charge le sitemap depuis l'URL configur√©e"""
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
    }
    try:
        response = requests.get(SITEMAP_URL, headers=headers, timeout=HTTP_TIMEOUT)
        if response.status_code == 200:
            logging.info(f"Sitemap t√©l√©charg√© ({len(response.text)} caract√®res)")
            return response.text
        else:
            logging.error(f"Erreur t√©l√©chargement sitemap: HTTP {response.status_code}")
    except requests.exceptions.Timeout:
        logging.error("Timeout lors du t√©l√©chargement du sitemap")
    except requests.exceptions.RequestException as e:
        logging.error(f"Erreur r√©seau sitemap: {e}")
    return None


def parse_sitemap(xml_content):
    """Parse le contenu XML du sitemap et extrait les PDFs"""
    try:
        root = ET.fromstring(xml_content)
        namespace = "{http://www.sitemaps.org/schemas/sitemap/0.9}"
        pdfs = {}

        for url in root.findall(f"{namespace}url"):
            loc_elem = url.find(f"{namespace}loc")
            lastmod_elem = url.find(f"{namespace}lastmod")
            
            if loc_elem is not None and loc_elem.text:
                loc = loc_elem.text
                lastmod = lastmod_elem.text if lastmod_elem is not None else ""
                pdfs[loc] = lastmod

        logging.info(f"Sitemap pars√©: {len(pdfs)} URLs trouv√©es")
        return pdfs
    except ET.ParseError as e:
        logging.error(f"Erreur parsing XML sitemap: {e}")
        return {}


def save_sitemap(xml_content):
    """Sauvegarde le sitemap localement"""
    try:
        with open(LOCAL_SITEMAP_FILE, "w", encoding="utf-8") as f:
            f.write(xml_content)
        logging.info(f"Sitemap sauvegard√©: {LOCAL_SITEMAP_FILE}")
    except Exception as e:
        logging.error(f"Erreur sauvegarde sitemap: {e}")


def load_local_sitemap():
    """Charge le sitemap local s'il existe"""
    if not os.path.exists(LOCAL_SITEMAP_FILE):
        logging.info("Pas de sitemap local existant")
        return {}

    try:
        with open(LOCAL_SITEMAP_FILE, "r", encoding="utf-8") as f:
            return parse_sitemap(f.read())
    except Exception as e:
        logging.error(f"Erreur lecture sitemap local: {e}")
        return {}


def compare_sitemaps(old_pdfs, new_pdfs):
    """Compare deux sitemaps et retourne les changements"""
    added = {url: date for url, date in new_pdfs.items() if url not in old_pdfs}
    changed = {url: date for url, date in new_pdfs.items() if url in old_pdfs and old_pdfs[url] != date}
    removed = {url: old_pdfs[url] for url in old_pdfs if url not in new_pdfs}
    
    logging.info(f"Comparaison sitemap: {len(added)} ajout√©s, {len(changed)} modifi√©s, {len(removed)} supprim√©s")
    
    return added, changed, removed


# ============================================
# GESTION DES PDFS SUPPRIM√âS
# ============================================

def handle_removed_pdfs(removed_urls):
    """
    G√®re les PDFs supprim√©s du sitemap avec nettoyage complet:
    1. Suppression de la source chatbot
    2. Suppression du fichier markdown local
    3. Suppression du fichier markdown sur FTP
    4. Mise √† jour du tracking JSON
    """
    if not removed_urls:
        logging.info("Aucun PDF supprim√© √† traiter")
        return
    
    logging.info(f"{'='*50}")
    logging.info(f"TRAITEMENT DES {len(removed_urls)} PDF(S) SUPPRIM√â(S)")
    logging.info(f"{'='*50}")
    
    # R√©cup√©rer les sources du chatbot une seule fois
    sources = get_sources()
    if sources is None:
        logging.warning("Impossible de r√©cup√©rer les sources du chatbot - nettoyage partiel")
    
    success_count = 0
    partial_count = 0
    
    for url in removed_urls:
        clean_filename = get_clean_filename(url)
        md_filename = get_markdown_filename(url)
        
        logging.info(f"\nüóëÔ∏è Nettoyage: {clean_filename}")
        
        cleanup_result = {
            "chatbot_source": False,
            "local_file": False,
            "ftp_file": False,
            "tracking": False
        }
        
        # 1. Supprimer la source du chatbot
        if sources:
            source = find_source_by_keyword(sources, clean_filename)
            if source:
                if delete_source(source["id"]):
                    logging.info(f"  ‚úì Source chatbot supprim√©e: {source['id']}")
                    cleanup_result["chatbot_source"] = True
                else:
                    logging.warning(f"  ‚úó √âchec suppression source chatbot: {source['id']}")
            else:
                logging.info(f"  ‚óã Pas de source chatbot trouv√©e")
                cleanup_result["chatbot_source"] = True  # Pas d'erreur, juste absent
        
        # 2. Supprimer le fichier markdown local
        md_path = get_markdown_path(url)
        if os.path.exists(md_path):
            try:
                os.remove(md_path)
                logging.info(f"  ‚úì Fichier local supprim√©: {md_path}")
                cleanup_result["local_file"] = True
            except Exception as e:
                logging.warning(f"  ‚úó Impossible de supprimer le fichier local: {e}")
        else:
            logging.info(f"  ‚óã Fichier local d√©j√† absent")
            cleanup_result["local_file"] = True  # Pas d'erreur, juste absent
        
        # 3. Supprimer le fichier sur FTP
        if delete_from_ftp(md_filename):
            logging.info(f"  ‚úì Fichier FTP supprim√©: {md_filename}")
            cleanup_result["ftp_file"] = True
        else:
            logging.warning(f"  ‚úó √âchec suppression fichier FTP")
        
        # 4. Retirer du tracking JSON (processed_pdfs.json)
        if remove_processed_pdf(url):
            logging.info(f"  ‚úì Retir√© du tracking (processed)")
            cleanup_result["tracking"] = True
        else:
            logging.info(f"  ‚óã Pas dans le tracking (processed)")
            cleanup_result["tracking"] = True
        
        # 5. Retirer aussi de failed_pdfs.json si pr√©sent
        failed = load_failed_pdfs()
        if url in failed:
            del failed[url]
            save_failed_pdfs(failed)
            logging.info(f"  ‚úì Retir√© du tracking (failed)")
        
        # 6. Enregistrer dans l'historique des suppressions
        save_removed_pdf(url, cleanup_result)
        
        # √âvaluer le r√©sultat
        all_success = all(cleanup_result.values())
        any_success = any(cleanup_result.values())
        
        if all_success:
            logging.info(f"  ‚úÖ Nettoyage complet r√©ussi")
            success_count += 1
        elif any_success:
            logging.warning(f"  ‚ö†Ô∏è Nettoyage partiel")
            partial_count += 1
        else:
            logging.error(f"  ‚ùå Nettoyage √©chou√©")
    
    logging.info(f"\n{'='*50}")
    logging.info(f"R√âSUM√â SUPPRESSIONS: {success_count} complets, {partial_count} partiels, {len(removed_urls) - success_count - partial_count} √©chou√©s")
    logging.info(f"{'='*50}")


# ============================================
# FONCTIONS PDF
# ============================================

def download_pdf(url):
    """T√©l√©charge un PDF depuis l'URL"""
    clean_filename = get_clean_filename(url)
    filepath = os.path.join(DOWNLOAD_FOLDER, clean_filename)

    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
    }
    
    try:
        response = requests.get(url, headers=headers, stream=True, timeout=HTTP_TIMEOUT)
        
        if response.status_code == 200:
            total_size = int(response.headers.get('content-length', 0))
            
            with open(filepath, "wb") as f:
                downloaded = 0
                for chunk in response.iter_content(chunk_size=8192):
                    f.write(chunk)
                    downloaded += len(chunk)
            
            file_size = os.path.getsize(filepath)
            logging.info(f"T√©l√©charg√©: {clean_filename} ({file_size / 1024:.1f} KB)")
            
            # V√©rifier que le fichier n'est pas vide
            if file_size < 100:
                raise Exception(f"Fichier PDF trop petit ({file_size} bytes), probablement corrompu")
            
            return filepath
        else:
            raise Exception(f"HTTP {response.status_code}")
            
    except requests.exceptions.Timeout:
        raise Exception("Timeout lors du t√©l√©chargement")
    except requests.exceptions.RequestException as e:
        raise Exception(f"Erreur r√©seau: {e}")


def get_converter():
    """Retourne ou cr√©e le converter PDF (singleton pour r√©utilisation)"""
    global _converter
    
    if _converter is None:
        logging.info("Initialisation du converter Marker...")
        config = {
            "output_format": "markdown",
            "languages": "fr",
            "disable_image_extraction": True,
            "max_tasks_per_worker": 1
        }
        
        config_parser = ConfigParser(config)
        _converter = PdfConverter(
            config=config_parser.generate_config_dict(),
            artifact_dict=create_model_dict(),
            processor_list=config_parser.get_processors(),
            renderer=config_parser.get_renderer()
        )
        logging.info("Converter initialis√©")
    
    return _converter


def convert_pdf_to_markdown(pdf_path, source_url):
    """Convertit un PDF en Markdown"""
    clean_filename = get_clean_filename(source_url)
    md_filename = os.path.join(MARKDOWN_FOLDER, clean_filename.replace(".pdf", ".md"))
    
    try:
        converter = get_converter()
        rendered = converter(pdf_path)
        text, _, _ = text_from_rendered(rendered)
        
        # V√©rifier que la conversion a produit du contenu
        if not text or len(text.strip()) < 50:
            raise Exception("Conversion produit un contenu vide ou trop court")
        
        # Cr√©er le titre propre
        clean_title = clean_filename.replace("-", " ").replace(".pdf", "")
        
        # √âcrire le fichier markdown
        with open(md_filename, "w", encoding="utf-8") as f:
            f.write(text)
            f.write(f"\n\n---\n\n**Source :** [{clean_title}]({source_url})")
        
        logging.info(f"Converti en Markdown: {clean_filename} ({len(text)} caract√®res)")
        
        # Upload FTP
        if not upload_to_ftp(md_filename):
            logging.warning("Upload FTP √©chou√©, mais on continue")
        
        # Int√©gration chatbot
        process_chatbot_source(source_url)
        
    except Exception as e:
        logging.error(f"Erreur conversion {clean_filename}: {e}")
        raise
    finally:
        cleanup_gpu_memory()


def process_pdf(url, date):
    """Traite un PDF complet: t√©l√©chargement, conversion, upload"""
    clean_filename = get_clean_filename(url)
    logging.info(f"{'='*50}")
    logging.info(f"Traitement: {clean_filename}")
    logging.info(f"Date sitemap: {date}")
    logging.info(f"URL: {url}")
    
    pdf_path = None
    
    try:
        # V√©rifier la m√©moire avant de commencer
        if not check_memory_usage():
            raise Exception("M√©moire insuffisante pour traiter ce PDF")
        
        # T√©l√©charger le PDF
        pdf_path = download_pdf(url)
        
        # Convertir en Markdown
        convert_pdf_to_markdown(pdf_path, url)
        
        # Marquer comme trait√©
        save_processed_pdf(url, date)
        
        # Retirer de la liste des √©checs si pr√©sent
        remove_from_failed(url)
        
        logging.info(f"‚úÖ SUCCESS: {clean_filename}")
        return True
        
    except Exception as e:
        error_msg = f"Erreur traitement {clean_filename}: {e}"
        logging.error(f"‚ùå FAILED: {error_msg}")
        save_failed_pdf(url, str(e))
        return False
    
    finally:
        # Nettoyer le fichier PDF temporaire
        if pdf_path and os.path.exists(pdf_path):
            try:
                os.remove(pdf_path)
                logging.debug(f"Fichier temporaire supprim√©: {pdf_path}")
            except Exception as e:
                logging.warning(f"Impossible de supprimer {pdf_path}: {e}")


# ============================================
# FONCTION PRINCIPALE
# ============================================

def main():
    logging.info("=" * 60)
    logging.info("D√âMARRAGE DU SCRIPT PDF CONVERTER")
    logging.info(f"Date/Heure: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    logging.info("=" * 60)
    
    # Acqu√©rir le lock
    if not acquire_lock():
        logging.error("Une autre instance est d√©j√† en cours d'ex√©cution. Arr√™t.")
        sys.exit(1)
    
    # Enregistrer la lib√©ration du lock √† la fin
    atexit.register(release_lock)
    
    # V√©rifier les variables d'environnement
    required_vars = [SITEMAP_URL, LOCAL_SITEMAP_FILE, DOWNLOAD_FOLDER, MARKDOWN_FOLDER, 
                     FTP_HOST, FTP_USER, FTP_PASS, CHATBOT_ID, BEARER_TOKEN, BASE_URL]
    if not all(required_vars):
        logging.error("Variables d'environnement manquantes!")
        upload_to_ftp("logs.log")
        suspendInstance()
        sys.exit(1)
    
    # Cr√©er les dossiers si n√©cessaire
    os.makedirs(DOWNLOAD_FOLDER, exist_ok=True)
    os.makedirs(MARKDOWN_FOLDER, exist_ok=True)
    
    # T√©l√©charger le nouveau sitemap
    new_sitemap_content = download_sitemap()
    if not new_sitemap_content:
        logging.error("Impossible de t√©l√©charger le sitemap")
        upload_to_ftp("logs.log")
        suspendInstance()
        sys.exit(1)
    
    # Parser les sitemaps
    new_pdfs = parse_sitemap(new_sitemap_content)
    old_pdfs = load_local_sitemap()
    
    # Comparer
    added, changed, removed = compare_sitemaps(old_pdfs, new_pdfs)
    
    # ============================================
    # √âTAPE 1: G√©rer les PDFs supprim√©s
    # ============================================
    handle_removed_pdfs(removed)
    
    # ============================================
    # √âTAPE 2: Traiter les nouveaux/modifi√©s
    # ============================================
    
    # Filtrer les PDFs √† traiter
    to_process = {}
    for url, date in {**added, **changed}.items():
        # V√©rifier si d√©j√† trait√© avec la m√™me date
        if is_pdf_already_processed(url, date):
            logging.debug(f"D√©j√† trait√©, ignor√©: {get_clean_filename(url)}")
            continue
        
        # V√©rifier si en √©chec multiple
        if not should_retry_failed_pdf(url):
            logging.debug(f"√âchec multiple, ignor√©: {get_clean_filename(url)}")
            continue
        
        to_process[url] = date
    
    total_pdfs = len(to_process)
    logging.info(f"\n{'='*50}")
    logging.info(f"PDFs √Ä TRAITER: {total_pdfs}")
    logging.info(f"{'='*50}")
    
    if total_pdfs == 0:
        logging.info("Aucun PDF √† traiter")
    else:
        processed_count = 0
        failed_count = 0
        
        for idx, (url, date) in enumerate(to_process.items(), 1):
            logging.info(f"\n[{idx}/{total_pdfs}] D√©but traitement...")
            
            success = process_pdf(url, date)
            
            if success:
                processed_count += 1
            else:
                failed_count += 1
            
            logging.info(f"Progression: {processed_count} r√©ussis, {failed_count} √©chou√©s sur {idx} trait√©s")
            
            # Pause entre les PDFs pour √©viter la surcharge
            if idx < total_pdfs:
                time.sleep(5)
        
        logging.info(f"\n{'='*50}")
        logging.info(f"R√âSUM√â TRAITEMENT: {processed_count}/{total_pdfs} PDFs trait√©s avec succ√®s")
        if failed_count > 0:
            logging.warning(f"‚ö†Ô∏è {failed_count} PDF(s) en √©chec (voir {FAILED_PDF_LOG})")
    
    # Sauvegarder le sitemap
    save_sitemap(new_sitemap_content)
    
    # Stats finales
    end_time = time.time()
    execution_time = end_time - start_time
    
    logging.info(f"\n{'='*60}")
    logging.info(f"FIN DU SCRIPT")
    logging.info(f"Temps d'ex√©cution: {execution_time:.2f} secondes ({execution_time/60:.1f} minutes)")
    logging.info(f"{'='*60}")
    
    # Upload des logs et fichiers de tracking
    upload_to_ftp("logs.log")
    if os.path.exists(PROCESSED_PDF_LOG):
        upload_to_ftp(PROCESSED_PDF_LOG)
    if os.path.exists(FAILED_PDF_LOG):
        upload_to_ftp(FAILED_PDF_LOG)
    if os.path.exists(REMOVED_PDF_LOG):
        upload_to_ftp(REMOVED_PDF_LOG)
    
    # Suspendre l'instance
    suspendInstance()


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        logging.info("Interruption utilisateur (Ctrl+C)")
        release_lock()
        sys.exit(0)
    except Exception as e:
        logging.critical(f"Erreur fatale non g√©r√©e: {e}", exc_info=True)
        upload_to_ftp("logs.log")
        release_lock()
        suspendInstance()
        sys.exit(1)
